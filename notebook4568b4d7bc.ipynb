{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndt=pd.read_csv('../input/swcjob/trainData.csv')\nX_test=pd.read_csv('../input/swcjob/testData.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=dt['target']\ny=y-1\nX=dt.drop(['target'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.corr()[103:104].max().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v101 v96 v94 v14 v75 v23 v7 v28 v74 v39 v61 v36 v66 v15 v47","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1=X['v101']*X['v96']\n#a2=X['v101']*X['v94']\n#a3=X['v101']*X['v14']\n#a4=X['v101']*X['v75']\n#a5=X['v101']*X['v23']\n#a6=X['v101']*X['v7']a7=X['v101']*X['v28']a8=X['v101']*X['v74']a9=X['v101']*X['v39']a10=X['v101']*X['v61']a11=X['v101']*X['v36']a12=X['v101']*X['v66']\n#a13=X['v101']*X['v15']a14=X['v101']*X['v47']a13=X['v94']+X['v14']a11=X['v75']+X['v23']\na12=X['v47']*X['v15']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.concat([X,a1,a12], axis=1)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m=X.mean(axis = 1, skipna = True) \ns=X.std(axis = 1, skipna = True)\na=(X == 0).astype(int).sum(axis=1)\nb1=(X > 0.1).astype(int).sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfor a in range(30):\n X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=(0.99-0.01*a), test_size=(0.01+0.01*a),  random_state=0)\n clf = RandomForestClassifier(n_estimators=40)\n clf.fit(X_train, y_train)\n clf_probs = clf.predict_proba(X_valid)\n score = log_loss(y_valid, clf_probs)\n print(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pd.concat([X,m,s,a,b1], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.scatter(y,X['v1'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport optuna\n\n\n# FYI: Objective functions can take additional arguments\n# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\ndef objective(trial):\n   \n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n\n    param = {\n        \"objective\": \"multi:softprob\",\n        \"num_class\": 9,  \n         'nthread': 4,\n        \"eta\" : trial.suggest_uniform('eta', 0.23, 0.27),\n        \"max_depth\" : trial.suggest_int('max_depth', 20, 50),\n        \"colsample_bytree\" :trial.suggest_discrete_uniform('colsample_bytree',0.5,1.0,0.05),\n        \"reg_lambda\" : trial.suggest_int('reg_lambda', 1, 6),\n        \"reg_alpha\" : trial.suggest_int('reg_alpha', 0, 3),\n        \"subsample\" : trial.suggest_discrete_uniform('subsample', 0.6, 0.9, 0.1),\n       \n         'gamma' : trial.suggest_discrete_uniform('gamma', 0, 0.50,0.05)\n       \n    }\n\n   \n \n    bst = xgb.train(param, dtrain,50)\n    preds = bst.predict(dvalid)\n    from sklearn.metrics import log_loss\n\n    score = log_loss(y_valid, preds)\n\n    #pred_labels = np.rint(preds)\n    #accuracy = sklearn.metrics.accuracy_score(y_valid, pred_labels)\n    return score\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=10)\n    print(study.best_trial)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"50{'eta': 0.23762755885384773, 'max_depth': 29, 'colsample_bytree': 0.6, 'reg_lambda': 3, 'reg_alpha': 1, 'subsample': 0.9, 'gamma': 0.05}. Best is trial 6 with value: 0.568241580962883\n0.5492 params={'eta': 0.5498330201572368, 'max_depth': 46, 'colsample_bytree': 0.8500000000000001, 'reg_lambda': 2, 'reg_alpha': 1, 'subsample': 0.9, 'gamma': 0.45},\nTrial 44 finished with value: 0.5430509707716035 and parameters: {'eta': 0.6536434630511088, 'max_depth': 19, 'colsample_bytree': 0.9, 'reg_lambda': 3, 'reg_alpha': 1, 'subsample': 0.9}. Best is trial 44 with value: 0.5430509707716035.\nTrial 18 finished with value: 0.5533558963900894 and parameters: {'eta': 0.5549795583702813, 'max_depth': 40, 'colsample_bytree': 0.8500000000000001, 'reg_lambda': 3, 'reg_alpha': 1, 'subsample': 0.8}. Best is trial 18 with value: 0.5533558963900894.\n Trial 45 finished with value: 0.5494765198186428 and parameters: {'eta': 0.563997789058819, 'max_depth': 32, 'colsample_bytree': 0.8500000000000001, 'reg_lambda': 5, 'reg_alpha': 0, 'subsample': 0.8}. Best is trial 45 with value: 0.5494765198186428. \nTrial 14 finished with value: 0.5423197635309989 and parameters: {'eta': 0.6232288811010834, 'max_depth': 29, 'colsample_bytree': 1.0, 'reg_lambda': 5, 'reg_alpha': 0, 'subsample': 0.9, 'n_estimators': 110, 'gamma': 0.0}\n nr=50    Trial 1 finished with value: 0.4903807516219084 and parameters: {'eta': 0.24000312574308566, 'max_depth': 25, 'colsample_bytree': 0.9, 'reg_lambda': 5, 'reg_alpha': 3, 'subsample': 0.8, 'gamma': 0.45}. Best is trial 1 with value: 0.4903807516219084.\nnr=50    Trial 11 finished with value: 0.48837454010783576 and parameters: {'eta': 0.24389135031363707, 'max_depth': 18, 'colsample_bytree': 1.0, 'reg_lambda': 1, 'reg_alpha': 2, 'subsample': 0.7, 'gamma': 0.35000000000000003}. Best is trial 11 with value: 0.48837454010783576.\nnr=50  Trial 23 finished with value: 0.4881052793388139 and parameters: {'eta': 0.38492946294074404, 'max_depth': 19, 'colsample_bytree': 0.8500000000000001, 'reg_lambda': 4, 'reg_alpha': 2, 'subsample': 0.9, 'gamma': 0.45}. Best is trial 23 with value: 0.4881052793388139.   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport sklearn.datasets\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport optuna.integration.lightgbm as lgb\n\n\ndtrain = lgb.Dataset(X_train, label=y_train)\ndval = lgb.Dataset(X_valid, label=y_valid)\n\nparams = {\n        \"objective\": \"multiclass\",\n        \"metric\": \"multi_logloss\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n       'num_class':9\n    }\nmodel = lgb.train( params, dtrain, valid_sets=[dtrain, dval],verbose_eval=100, early_stopping_rounds=100  )\n\nprediction = model.predict(X_valid, num_iteration=model.best_iteration)\n\nfrom sklearn.metrics import log_loss\nscore = log_loss(y_valid, prediction)\n\nbest_params = model.params\nprint(\"Best iteration: \", model.best_iteration)\nprint(\"Best params:\", best_params)\nprint(\"  Logloss = {}\".format(score))\nprint(\"  Params: \")\nfor key, value in best_params.items():\n        print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" model.best_learning_rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Best iteration:  275\nBest params: {'objective': 'multiclass', 'metric': 'multi_logloss', 'verbosity': -1, 'boosting_type': 'gbdt', 'num_class': 9, 'lambda_l1': 4.615246179203526e-05, 'lambda_l2': 1.4543790963930108e-05, 'num_leaves': 66, 'feature_fraction': 0.4, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20}\n Logloss = 0.4540108242157832","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_iteration=362\nBest params: {'objective': 'multiclass', 'metric': 'multi_logloss', 'verbosity': -1, 'boosting_type': 'gbdt', 'num_class': 9, 'lambda_l1': 1.0507017383691772e-08, 'lambda_l2': 9.463684298452806, 'num_leaves': 92, 'feature_fraction': 0.48000000000000004, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20}\n  Logloss = 0.45449985686616146\nmodel.best_iteration=277\nBest params: {'objective': 'multiclass', 'metric': 'multi_logloss', 'verbosity': -1, 'boosting_type': 'gbdt', 'num_class': 9, 'lambda_l1': 0.9659024503100162, 'lambda_l2': 4.940483637673107e-07, 'num_leaves': 101, 'feature_fraction': 0.4, 'bagging_fraction': 0.912017962284575, 'bagging_freq': 1, 'min_child_samples': 100}\n  Accuracy = 0.45603195405507707\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport catboost as cb\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier, Pool\nimport optuna\nfrom hyperopt import hp, fmin, tpe\n\ndef objective(trial):\n\n    param = {\n            'learning_rate': hp.loguniform('learning_rate', -3, 0),\n            'depth': hp.quniform('depth', 5, 14, 1),\n            'random_strength': hp.choice('random_strength', [1, 20]),\n            'l2_leaf_reg': hp.loguniform('l2_leaf_reg', 0, np.log(10)),\n            'bootstrap_type': hp.choice(\n                'bootstrap_type',\n                [\n                    {'type':'Bayesian',\n                     'bagging_temperature': hp.uniform('bagging_temperature', 0, 1)\n                    },\n                    {'type':'Bernoulli'}\n                ]\n            ),\n            'one_hot_max_size': hp.quniform('one_hot_max_size', 2, 255, 1),\n            #'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),\n            'boosting_type': hp.choice('boosting_type', ['Plain', 'Ordered'])\n    }\n\n    \n    cat = cb.CatBoostClassifier(**param, loss_function='MultiClass')\n\n    cat.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=100, early_stopping_rounds=100)\n\n    preds = cat.predict(X_valid)\n\n    from sklearn.metrics import log_loss\n    y_predC=cat.predict_proba(X_valid)\n    score = log_loss(y_valid, y_predC)\n    \n    return score\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=10, timeout=6000)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"500-Trial 4 finished with value: 0.483722351262301 and parameters: {'colsample_bylevel': 0.05068349978481952, 'depth': 10, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'subsample': 0.6139907470641855}. Best is trial 4 with value: 0.483722351262301.\nTrial 0 finished with value: 0.47022622074273396 and parameters: {'colsample_bylevel': 0.07857755347002501, 'depth': 9, 'boosting_type': 'Ordered', 'bootstrap_type': 'Bernoulli', 'subsample': 0.8950286882413444}. Best is trial 0 with value: 0.47022622074273396.\nTrial 0 finished with value: 0.4764491880906449 and parameters: {'colsample_bylevel': 0.06227927372827783, 'depth': 9, 'boosting_type': 'Plain', 'bootstrap_type': 'Bayesian', 'bagging_temperature': 1.6846173304677314}. Best is trial 0 with value: 0.4764491880906449.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.datasets import load_breast_cancer,load_boston,load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import mean_squared_error,roc_auc_score,precision_score\n\nd_train=lgb.Dataset(X_train, label=y_train)\nparams={}\n\nparams['boosting_type']='gbdt' #GradientBoostingDecisionTree\nparams['objective']='multiclass' #Multi-class target feature\nparams['metric']='multi_logloss' #metric for multi-class\nparams['learning_rate']=0.08\nparams['max_depth']=50\nparams['num_class']=9\nparams['subsample']=0.3\nparams['colsample_bytree']=0.3\nparams['reg_alpha']=0.4\nparams['reg_lambda']=0.4\nparams['min_child_samples']= 60\n#params['num_leaves']=30\nclf=lgb.train(params,d_train,800, early_stopping_rounds=100)\nfrom sklearn.metrics import log_loss\ny_predL=clf.predict(X_valid)\nscore = log_loss(y_valid, y_predL)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predL2=clf.predict(X_test)\n#dtest= xgb.DMatrix(X_test)\n#y_predX2=bst.predict(dtest)\ny_redC2=cat.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predL=y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in range (0,25):\n y_predT=0.01*a*y_predC+(0.25-0.01*a)*y_predX+0.75*y_predL\n from sklearn.metrics import log_loss\n score = log_loss(y_valid, y_predT)\n print(str(a)+\" \"+ str(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_predT2=0.75*y_predL2+0.25*y_redC2\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport xgboost as xgb\nfrom sklearn.datasets import dump_svmlight_file\nfrom sklearn.metrics import precision_score\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_valid, label=y_valid)\n\n# set xgboost params\nparam = {\n    'objective': 'multi:softprob',  # error evaluation for multiclass training\n    'num_class': 9,\n    'nthread': 8,\n    'eta': 0.1, 'max_depth': 18, 'colsample_bytree': 1.0, 'reg_lambda': 1, 'reg_alpha': 2, 'subsample': 0.7, 'gamma': 0.35000000000000003\n   } # the number of training iterations\nstart = time.time()\n#------------- numpy array ------------------\n# training and testing - numpy matrices\nbst = xgb.train(param, dtrain,300)\nfrom sklearn.metrics import log_loss\ny_predX=bst.predict(dvalid)\nscore = log_loss(y_valid, y_predX)\nprint(score)\nfinish=time.time()\nprint(finish-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predT2=pd.DataFrame(y_predT2)\ny_predT2.to_csv (r'out1409.csv', index = False, header=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=y_predT2.sum(axis = 1, skipna = True) \na.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.datasets import load_breast_cancer,load_boston,load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import mean_squared_error,roc_auc_score,precision_score\nd_train=lgb.Dataset(X_train, label=y_train)\ndval = lgb.Dataset(X_valid, label=y_valid)\nparams={'learning_rate':0.05,'objective': 'multiclass', 'metric': 'multi_logloss', 'verbosity': -1, 'boosting_type': 'gbdt', 'num_class': 9, 'lambda_l1': 4.615246179203526e-05, 'lambda_l2': 1.4543790963930108e-05, 'num_leaves': 66, 'feature_fraction': 0.4, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20}\nclf=lgb.train(params,d_train,800, verbose_eval=200 )\nfrom sklearn.metrics import log_loss\ny_pred=clf.predict(X_valid)\nscore = log_loss(y_valid, y_pred)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"0.9T\n0.03 0.4506\n0.02  0.4480\n0.015 0.4494\n0.02     0.4483 1800","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import log_loss\nclf = RandomForestClassifier(n_estimators=250)\nclf.fit(X_train, y_train)\nclf_probs = clf.predict_proba(X_valid)\nscore = log_loss(y_valid, clf_probs)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cb\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nparams = {'depth': [5,10],\n          'learning_rate' : [ 0.08],    \n         'iterations': [200,500]}\ncbm = cb.CatBoostClassifier()\ncbm2 = GridSearchCV(cbm, params, scoring='neg_log_loss', cv = 2,verbose=100,n_jobs=-1)\ncbm2.fit(X_train, y_train)\nprint(cbm2.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cb\nfrom sklearn import metrics\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.datasets import load_breast_cancer,load_boston,load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import mean_squared_error,roc_auc_score,precision_score\neval_dataset = Pool(X_valid,y_valid)\npr= {}\ncat = cb.CatBoostClassifier( learning_rate=0.15,loss_function='MultiClass',colsample_bylevel= 0.05068349978481952,depth= 10,boosting_type='Ordered', bootstrap_type='Bernoulli',subsample= 0.6139907470641855)  \ncat.fit(X_train, y_train, eval_set=eval_dataset ,early_stopping_rounds=100,verbose=100)\n    \nfrom sklearn.metrics import log_loss\ny_predC=cat.predict_proba(X_valid)\nscore = log_loss(y_valid, y_predC)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"0.1 47.37\n0.15 46.8\n0.2 47\n0.3 48","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cb\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nparams = {'depth': [5,8,10],\n          'learning_rate' : [ 0.01,0.1,0.2],    \n         'iterations': [200,500,1000],\n         'reg_lambda': [ 1, 10]\n   }\ncb = cb.CatBoostClassifier()\ncb_model = GridSearchCV(cb, params, scoring='neg_log_loss', cv = 2,n_jobs=-1)\ncb_model.fit(X_train, y_train,early_stopping_rounds=100,verbose=100)\nprint(cb_model.best_params_)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}